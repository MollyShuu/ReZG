{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d913951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T14:59:20.991994Z",
     "start_time": "2024-10-13T14:59:18.848327Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# (0)Import dependency package\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d15abed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:00:08.515895Z",
     "start_time": "2024-10-13T15:00:08.504186Z"
    },
    "code_folding": [
     0,
     1,
     19,
     28,
     42,
     117
    ]
   },
   "outputs": [],
   "source": [
    "# (1) Define used fuctions\n",
    "class RobertaC2(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_classes:int , gpt_model_name:str):\n",
    "        super(RobertaC2,self).__init__()\n",
    "        self.roberta_model = RobertaModel.from_pretrained(gpt_model_name)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size,num_classes)\n",
    "        )\n",
    "       # self.fc1 = nn.Sequential(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        \n",
    "    def forward(self,input_id, mask):\n",
    "        \"\"\"\n",
    "        input_id: encoded inputs ids of sent.\n",
    "        \"\"\"\n",
    "        last_seq = self.roberta_model(input_ids=input_id, attention_mask=mask).last_hidden_state[:, 0, :]\n",
    "        #print(last_seq,last_seq.size()) # torch.Size([bs, 1024])\n",
    "        linear_output = self.fc1(last_seq)\n",
    "        return linear_output\n",
    "def set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)    \n",
    "def evaluate_loss(net, device, criterion, val_dataloader):\n",
    "    net.eval()\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for val_input, val_label in val_dataloader:\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_input['attention_mask'].to(device)\n",
    "            input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
    "            logits = net(input_id,mask)\n",
    "            mean_loss += criterion(logits, val_label).item()\n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count            \n",
    "def train_roberta(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it,(train_input, train_label) in enumerate(tqdm(train_loader)):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "            with autocast():\n",
    "                # Obtaining the logits from the model\n",
    "                logits = net(input_id,mask)\n",
    "\n",
    "                # Computing loss\n",
    "                loss = criterion(logits, train_label)\n",
    "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
    "\n",
    "            # Backpropagating the gradients\n",
    "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "           # print(loss,loss.is_cuda)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (it + 1) % iters_to_accumulate == 0:\n",
    "                # Optimization step\n",
    "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
    "                # otherwise, opti.step() is skipped.\n",
    "                scaler.step(opti)\n",
    "                # Updates the scale for next iteration.\n",
    "                scaler.update()\n",
    "                # Adjust the learning rate based on the number of iterations.\n",
    "                lr_scheduler.step()\n",
    "                # Clear gradients\n",
    "                opti.zero_grad()\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "    # Saving the model\n",
    "    path_to_model='./models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(model_name, lr, round(best_loss, 5), best_ep)\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()   \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df,tokenizer):\n",
    "        self.labels = list(df['label'])\n",
    "        self.texts = [tokenizer(tokenizer.cls_token+df['hs'][index]+tokenizer.sep_token+df['cn'][index]+tokenizer.sep_token,\n",
    "                        padding='max_length',\n",
    "                        max_length=256,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\") for index in range(len(df['index']))]\n",
    "#         self.texts = [tokenizer('[CLS]'+df['hs'][index]+'[SEP]'+df['cn'][index]+'[EOS]',\n",
    "#                                 padding='max_length',\n",
    "#                                 max_length=256,\n",
    "#                                 truncation=True,\n",
    "#                                 return_tensors=\"pt\") for index in range(len(df['index']))]\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        # Get a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "    \n",
    "    def get_batch_texts(self, idx):\n",
    "        # Get a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6801df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:06:41.645561Z",
     "start_time": "2024-10-13T15:06:41.588024Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# (2)Parameter setting\n",
    "data_files='./path/hs_cn_dataset.csv'   # the training and testing datasets\n",
    "save_path='./saved_models/'   #The path to save the finetuned models      \n",
    "model_name ='./path/roberta-large' # The path to store roberta-large\n",
    "hidden_size=1024\n",
    "freeze = False  # if True, freeze the encoder weights and only update the classification layer weights\n",
    "maxlen = 256  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
    "batch_size = 16  # 16 batch size\n",
    "iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
    "lr = 2e-5  # learning rate\n",
    "epochs = 4 # number of training epochs\n",
    "set_seed(42)\n",
    "tokenizer= RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6d51d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:22:52.549382Z",
     "start_time": "2024-10-13T15:22:52.546754Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# (3)Preprocess dataset\n",
    "datasets = load_dataset('csv', data_files=data_files, \n",
    "                        cache_dir=\"./data/\", delimiter=\",\")\n",
    "\n",
    "# Spliting datasets\n",
    "split = datasets['train'].train_test_split(test_size=0.1, seed=2,shuffle=True)  # split the original training data for validation\n",
    "train_data = split['train']  # 90 % of the original training data\n",
    "val_test = split['test']   # 10 % of the original training data\n",
    "split =val_test.train_test_split(test_size=0.5,seed=1)\n",
    "valid_data=split['train']\n",
    "test_data=split['test']\n",
    "\n",
    "# Transform data into pandas dataframes\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_val = pd.DataFrame(valid_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# Creating instances of training and validation set\n",
    "train= Dataset(df_train,tokenizer)\n",
    "val= Dataset(df_val,tokenizer)  \n",
    "print(\"Reading training data...\") \n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True,num_workers=5)\n",
    "print(\"Reading validation data...\")\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size,num_workers=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f456ebc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:17:17.053509Z",
     "start_time": "2024-10-13T15:17:17.051109Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# (4) Load training funtions\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = RobertaC2(hidden_size=hidden_size, num_classes=2, gpt_model_name=model_name)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
    "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
    "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
    "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9301b6d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:01:16.731351Z",
     "start_time": "2024-10-13T15:01:16.728100Z"
    },
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# (5) Starting training and save the best model.\n",
    "best_loss = np.Inf\n",
    "best_ep = 1\n",
    "nb_iterations = len(train_loader)\n",
    "print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "iters = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for ep in range(epochs):\n",
    "\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for it,(train_input, train_label) in enumerate(tqdm(train_loader)):\n",
    "        train_label = train_label.to(device)\n",
    "        mask = train_input['attention_mask'].to(device)\n",
    "        input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "        with autocast():\n",
    "            logits = net(input_id,mask)\n",
    "            loss = criterion(logits, train_label)\n",
    "            loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (it + 1) % iters_to_accumulate == 0:\n",
    "            scaler.step(opti)\n",
    "            scaler.update()\n",
    "            lr_scheduler.step()\n",
    "            opti.zero_grad()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (it + 1) % print_every == 0:  # Print training loss information\n",
    "            print()\n",
    "            print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                  .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "    val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "    print()\n",
    "    print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "        print()\n",
    "        net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "        best_loss = val_loss\n",
    "        best_ep = ep + 1\n",
    "\n",
    "# Saving the model\n",
    "path_to_model=save_path+'{}_lr_{}_val_loss_{}_ep_{}.pt'.format(model_name, lr, round(best_loss, 5), best_ep)\n",
    "torch.save(net_copy.state_dict(), path_to_model)\n",
    "print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "del loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b5618",
   "metadata": {
    "code_folding": [
     1,
     12
    ]
   },
   "outputs": [],
   "source": [
    "#(6) Evaluating\n",
    "def cn_detect_model(model_path):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    net = RobertaC2(hidden_size=1024, num_classes=2,\n",
    "                    gpt_model_name=model_name)  # RobertaSequenceClassifier\n",
    "    print(\"Loading the  weights of the model...\")\n",
    "    net.load_state_dict(torch.load(,model_path,map_location=device))\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    return net, tokenizer\n",
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data,tokenizer)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=8)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "        \n",
    "    # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    \n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "            \n",
    "            # add original labels\n",
    "            true_labels += test_label.cpu().numpy().flatten().tolist()\n",
    "            # get predicitons to list\n",
    "            predictions_labels += output.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    return true_labels, predictions_labels\n",
    "finetuned_model_path='\"./saved_models/CnDetection-ReZG.pt\"'\n",
    "net, tokenizer = cn_detect_model(finetuned_model_path)\n",
    "true_labels, pred_labels = evaluate(net, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsy_py3.7",
   "language": "python",
   "name": "jsy_py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
